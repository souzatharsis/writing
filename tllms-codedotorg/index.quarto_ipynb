{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Taming LLMs\"\n",
        "subtitle: \"A Practical Guide to LLM Pitfalls with Open Source Software\"\n",
        "description: | \n",
        "  A Practical Guide to LLM Pitfalls with Open Source Software\n",
        "date: 01/28/2025\n",
        "author:\n",
        "  - name: Th√°rsis Souza, Ph.D. \n",
        "citation:\n",
        "  url: https://www.souzatharsis.com/writing/tllms-codedotorg\n",
        "website:\n",
        "  repo-url: https://github.com/souzatharsis/writing/\n",
        "  repo-actions: [source, issue]\n",
        "repo-actions: true\n",
        "reference-location: margin\n",
        "citation-location: margin\n",
        "highlight-style: github\n",
        "#cap-location: margin\n",
        "link-citations: true\n",
        "bibliography: ../references.bib\n",
        "editor:\n",
        "  render-on-save: false\n",
        "format:\n",
        "  revealjs: \n",
        "    theme: dark\n",
        "    slide-number: true\n",
        "    logo: images/taming.ico\n",
        "    footer: '[tamingllms.com](tamingllms.com)'\n",
        "---\n",
        "\n",
        "\n",
        "## About Me{auto-animate=true auto-animate-easing=\"ease-in-out\"}\n",
        "\n",
        "::: {.r-stack}\n",
        "::: {data-id=\"box1\" style=\"background: #e83e8c; width: 350px; height: 350px; border-radius: 200px;\"}\n",
        "CS\n",
        ":::\n",
        "::: {data-id=\"box2\" style=\"background: #3fb618; width: 250px; height: 250px; border-radius: 200px;\"}\n",
        "PM\n",
        ":::\n",
        "::: {data-id=\"box3\" style=\"background: #2780e3; width: 150px; height: 150px; border-radius: 200px;\"}\n",
        "Finance\n",
        ":::\n",
        ":::\n",
        "\n",
        "## About Me {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n",
        "\n",
        "::: {.r-hstack}\n",
        "::: {style=\"display: flex; flex-direction: column; align-items: center;\"}\n",
        "::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #e83e8c; width: 200px; height: 150px; margin: 10px; display: flex; justify-content: center; align-items: center;\"}\n",
        "**CS**\n",
        ":::\n",
        "::: {style=\"width: 2px; height: 30px; background: #e83e8c;\"}\n",
        ":::\n",
        "PhD, UCL\n",
        ":::\n",
        "\n",
        "::: {style=\"display: flex; flex-direction: column; align-items: center;\"}\n",
        "::: {data-id=\"box2\" auto-animate-delay=\"0.1\" style=\"background: #3fb618; width: 200px; height: 150px; margin: 10px; display: flex; justify-content: center; align-items: center;\"}\n",
        "**PM**\n",
        ":::\n",
        "::: {style=\"width: 2px; height: 30px; background: #3fb618;\"}\n",
        ":::\n",
        "SF\n",
        ":::\n",
        "\n",
        "::: {style=\"display: flex; flex-direction: column; align-items: center;\"}\n",
        "::: {data-id=\"box3\" auto-animate-delay=\"0.2\" style=\"background: #2780e3; width: 200px; height: 150px; margin: 10px; display: flex; justify-content: center; align-items: center;\"}\n",
        "**Finance**\n",
        ":::\n",
        "::: {style=\"width: 2px; height: 30px; background: #2780e3;\"}\n",
        ":::\n",
        "Ex-Two Sigma\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "1. LLM Pitfalls\n",
        "2. Case Study: Safety & Alignment\n",
        "3. Discussion\n",
        "\n",
        "## LLM Pitfalls\n",
        "\n",
        "![*Samuel Colvin, Pydantic*](images/bad.jpeg){#fig-bad}\n",
        "\n",
        "\n",
        "## LLM Pitfalls\n",
        "::: {.r-stack}\n",
        "::: {.r-hstack style=\"flex-wrap: wrap; justify-content: center; align-items: center; max-width: 1200px;\"}\n",
        "::: {.fragment style=\"background: #e83e8c; width: 300px; height: 150px; margin: 10px; padding: 15px; border-radius: 15px; display: flex; justify-content: center; align-items: center;\"}\n",
        "### Testing Complexity\n",
        ":::\n",
        "::: {.fragment style=\"background: #ff7518; width: 300px; height: 150px; margin: 10px; padding: 15px; border-radius: 15px; display: flex; justify-content: center; align-items: center;\"}\n",
        "### Structural (un)Reliability\n",
        ":::\n",
        "::: {.fragment style=\"background: #3fb618; width: 300px; height: 150px; margin: 10px; padding: 15px; border-radius: 15px; display: flex; justify-content: center; align-items: center;\"}\n",
        "### Input Data Issues\n",
        ":::\n",
        "::: {.fragment style=\"background: #2780e3; width: 300px; height: 150px; margin: 10px; padding: 15px; border-radius: 15px; display: flex; justify-content: center; align-items: center;\"}\n",
        "### Safety\n",
        ":::\n",
        "::: {.fragment style=\"background: #9954bb; width: 300px; height: 150px; margin: 10px; padding: 15px; border-radius: 15px; display: flex; justify-content: center; align-items: center;\"}\n",
        "### Alignment\n",
        ":::\n",
        "::: {.fragment style=\"background: #20c997; width: 300px; height: 150px; margin: 10px; padding: 15px; border-radius: 15px; display: flex; justify-content: center; align-items: center;\"}\n",
        "### Vendor <br> Lock-in\n",
        ":::\n",
        "::: {.fragment style=\"background: #6f42c1; width: 1000px; height: 150px; margin: 10px; padding: 15px; border-radius: 15px; display: flex; justify-content: center; align-items: center;\"}\n",
        "### Cost & Performance Optimization\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "::: {.r-fit-text style=\"background: #e83e8c; width: 100%; height: 100vh; margin: 0; padding: 15px; display: flex; justify-content: center; align-items: center; text-align: center; align-content: center;\"}\n",
        "### Testing Complexity\n",
        ":::\n",
        "\n",
        "\n",
        "## Testing Complexity\n",
        "\n",
        "LLMs are Generative, Non-Deterministic, and have Emergent Properties.\n",
        "\n",
        "![Animation representing LLMs \"Emerging Properties\" Phenomenon. From: \"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\"](images/emergent-properties-palm.gif){width=\"100%\"}\n",
        "\n",
        "\n",
        "\n",
        "## Testing Complexity {.smaller}\n",
        "\n",
        "| Aspect | Traditional Software Products | LLM-Based Software Products |\n",
        "|:--------|-------------------|------------------|\n",
        "| Capability Assessment | Validates specific functionality against requirements | May assess emergent properties like reasoning and creativity |\n",
        "| Metrics and Measurement | Precisely defined and measurable metrics | Subjective qualities that resist straightforward quantification |\n",
        "| Dataset Contamination | Uses carefully crafted test cases | Risk of memorized evaluation examples from training |\n",
        "| Benchmark Evolution | Maintains stable test suites | Continuously evolving benchmarks as capabilities advance |\n",
        "| Human Evaluation | Mostly automated validation | May require significant human oversight |\n",
        "\n",
        "## Testing Complexity: Evals Design\n",
        "\n",
        "![Conceptual overview of Multiple LLM-based applications evaluation.](images/conceptual-multi.svg){#fig-conceptual-multi}\n",
        "\n",
        "\n",
        "## Testing Complexity: Tools {.smaller}\n",
        "\n",
        "::: panel-tabset\n",
        "\n",
        "### LightEval\n",
        "\n",
        "```bash\n",
        "lighteval accelerate --model_args \"pretrained=meta-llama/Llama-3.2-1B-Instruct\" --tasks \"leaderboard|mmlu:econometrics|0|0\" --output_dir=\"./evals/\"\n",
        "```\n",
        "\n",
        "### LangSmith\n",
        "\n",
        "``` {.python}\n",
        "def run_evaluation(app, model_name, dataset,  evaluators):\n",
        "    results = langsmith_evaluate(\n",
        "        app,\n",
        "        data=dataset,\n",
        "        evaluators=evaluators,\n",
        "        experiment_prefix=model_name,\n",
        "        num_repetitions=5\n",
        "    )\n",
        "    return results\n",
        "```\n",
        "\n",
        "### PromptFoo\n",
        "\n",
        "```yaml\n",
        "description: Best model eval\n",
        "prompts:\n",
        "- file://prompt1.txt\n",
        "- file://prompt2.txt\n",
        "- file://prompt3.txt\n",
        "providers:\n",
        "- openai:gpt-3.5-turbo\n",
        "defaultTest:\n",
        "  assert:\n",
        "  - type: llm-rubric\n",
        "    value: 'Evaluate the output based on how detailed it is.  Grade it on a scale\n",
        "      of 0.0 to 1.0, where:\n",
        "\n",
        "      Score of 0.1: Not much detail.\n",
        "\n",
        "      Score of 0.5: Some detail.\n",
        "\n",
        "      Score of 1.0: Very detailed.\n",
        "\n",
        "      '\n",
        "tests: file://tests.csv\n",
        "```\n",
        ":::\n",
        "\n",
        "## Testing Complexity: Tools {.smaller}\n",
        "\n",
        "\n",
        "![](images/evals_compare.png){width=\"100%\"}\n",
        "\n",
        "\n",
        "\n",
        "## \n",
        "\n",
        "::: {.r-fit-text style=\"background: #ff7518; width: 100%; height: 100vh; margin: 0; padding: 15px; display: flex; justify-content: center; align-items: center; text-align: center; align-content: center;\"}\n",
        "### Structural (un)Reliability\n",
        ":::\n",
        "\n",
        "## Structural (un)Reliability\n",
        "\n",
        "::: incremental\n",
        "- Language Models excel at generating human-like text but struggle with producing structured output consistently [@tang2024strucbenchlargelanguagemodels; @shorten2024structuredragjsonresponseformatting]\n",
        "- This limitation poses significant challenges when integrating LLMs into production systems\n",
        "  - Databases\n",
        "  - APIs \n",
        "  - Other software applications\n",
        "- Even carefully crafted prompts cannot guarantee consistent structural adherence in LLM responses\n",
        ":::\n",
        "\n",
        "## Structural (un)Reliability {.smaller}\n",
        "\n",
        "But what user needs drive the demand for LLM output constraints? A recent Google Research [@10.1145/3613905.3650756] study explored this question through a survey of 51 industry professionals who use LLMs in their work:\n",
        "\n",
        "::: incremental\n",
        "- **Improving Developer Efficiency and Workflow**\n",
        "  - Reducing trial and error in prompt engineering\n",
        "  - Minimizing post-processing of LLM outputs \n",
        "  - Streamlining integration with downstream processes\n",
        "  - Enhancing quality of synthetic datasets\n",
        "\n",
        "- **Meeting UI and Product Requirements**\n",
        "  - Adhering to UI size limitations\n",
        "  - Ensuring output consistency\n",
        "\n",
        "- **Enhancing User Trust and Experience**\n",
        "  - Mitigating hallucinations\n",
        "  - Driving user adoption\n",
        ":::\n",
        "\n",
        "<!-- To constrain LLM output is not just a technical consideration but a fundamental user need, impacting developer efficiency and user experience. -->\n",
        "\n",
        "\n",
        "## Structural (un)Reliability {.smaller}\n",
        "\n",
        "The text generation process follows a probabilistic approach. At each step, the model calculates the probability distribution over its entire vocabulary to determine the most likely next token.\n",
        "\n",
        "![Text Generation Process: \"Sampling\".](images/logit.svg){#fig-logit}\n",
        "\n",
        "## Structural (un)Reliability {.smaller}\n",
        "\n",
        "This process can be expressed mathematically as:\n",
        "\n",
        "\\begin{equation}\n",
        "P(X) = P(x_1, x_2, \\ldots, x_n) = \\prod_{i=1}^n p(x_i|x_{<i})\n",
        "\\end{equation}\n",
        "\n",
        "where, $x_i$ represents the current token being generated, while $x_{<i}$ encompasses all preceding tokens.\n",
        "\n",
        "## Structural (un)Reliability {.smaller}\n",
        "\n",
        " This controlled text generation\\index{Controlled text generation} process can be formalized as [@liang2024controllabletextgenerationlarge]:\n",
        "\n",
        "\\begin{equation}\n",
        "P(X|\\color{green}{C}) = P(x_1, x_2, \\ldots, x_n|\\color{green}{C}) = \\prod_{i=1}^n p(x_i|x_{<i}, \\color{green}{C})\n",
        "\\end{equation}\n",
        "\n",
        "Here, $\\color{green}{C}$ represents the set of constraints or control conditions that shape the generated output.\n",
        "\n",
        "## Structural (un)Reliability {.smaller}\n",
        "\n",
        "Common constraints ($C$) include:\n",
        "\n",
        "::: incremental\n",
        "- **Format Constraints**: Enforcing specific output formats like JSON, XML, or YAML ensures the generated content follows a well-defined structure that can be easily parsed and validated. Format constraints are essential for system integration and data exchange.\n",
        "\n",
        "- **Multiple Choice Constraints**: Restricting LLM outputs to a predefined set of options helps ensure valid responses and reduces the likelihood of unexpected or invalid outputs. This is particularly useful for classification tasks or when specific categorical responses are required.\n",
        "\n",
        "- **Static Typing Constraints**: Enforcing data type requirements (strings, integers, booleans, etc.) ensures outputs can be safely processed by downstream systems. Type constraints help prevent runtime errors and improve system reliability.\n",
        "\n",
        "- **Length Constraints**: Limiting the length of generated content is crucial for UI display, platform requirements (like Twitter's character limit), and maintaining consistent user experience. Length constraints can be applied at the character, word, or token level.\n",
        "\n",
        "- **Ensuring Output Consistency**: Consistent output length and format are crucial for user experience and UI clarity. Constraints help maintain this consistency, avoiding overwhelming variability in generated text.\n",
        ":::\n",
        "\n",
        "## Structural (un)Reliability {.smaller}\n",
        "\n",
        "\n",
        "```{css}\n",
        "/*| echo: false */\n",
        "figcaption {\n",
        "  margin: auto;\n",
        "  text-align: center;\n",
        "}\n",
        "```\n",
        "\n",
        "![A common way to solve LLM Structural (un)Reliability.](images/x.png){#fig-x fig-align=\"center\"}\n",
        "\n",
        "## Structural (un)Reliability {.smaller}\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "81bba358"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}